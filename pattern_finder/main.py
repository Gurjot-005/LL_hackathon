import os
import numpy as np

from config import (
    DATA_PATH,
    PROCESSED_DATA_PATH,
    CORRELATION_THRESHOLD,
    OUTPUT_DIR
)

from data_handler import RawFileDataHandler
from cleaned_csv_handler import CleanedCSVFolderHandler
from loss_vector_builder import LossVectorBuilder
from correlation_engine import CorrelationEngine
from clustering_engine import ClusteringEngine
from confidence import compute_confidence
from visualization import Visualizer
from exporter import export_topology

# Nokia Challenge 2 Engine
from dual_capture_capacity_estimator import DualCaptureCapacityEstimator


# ===============================
# CONFIG: Switch data source
# ===============================
DATA_MODE = "raw"
# Change to "processed" when teammate finishes CSV pipeline


def main():
    print("üì° Nokia Fronthaul Pattern Finder\n")
    print("üîß Mode:", DATA_MODE.upper())
    print("üéöÔ∏è Correlation Threshold:", CORRELATION_THRESHOLD, "\n")

    # -------------------------------
    # Ensure output directory exists
    # -------------------------------
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    # -------------------------------
    # Load dataset (RAW or PROCESSED)
    # -------------------------------
    if DATA_MODE == "processed":
        handler = CleanedCSVFolderHandler(PROCESSED_DATA_PATH)
        dataset_label = "processed"
    else:
        handler = RawFileDataHandler(DATA_PATH)
        dataset_label = "raw"

    # -------------------------------
    # Discover cells
    # -------------------------------
    cells = handler.get_cells()
    print(f"Found {len(cells)} cells")

    if len(cells) < 2:
        print("‚ö†Ô∏è Not enough cells for topology inference")
        return

    # -------------------------------
    # Build behavior fingerprints
    # -------------------------------
    print("üß† Building behavior fingerprints...")
    vectors = LossVectorBuilder(handler).build()

    # Save for debugging / ML teammate
    np.save(os.path.join(OUTPUT_DIR, "loss_vectors.npy"), vectors)

    # -------------------------------
    # Correlation matrix
    # -------------------------------
    print("üìä Computing correlation matrix...")
    corr_engine = CorrelationEngine(CORRELATION_THRESHOLD)
    corr_df = corr_engine.compute_matrix(vectors)
    corr_df.to_csv(os.path.join(OUTPUT_DIR, "corr_matrix.csv"))

    # -------------------------------
    # Topology inference
    # -------------------------------
    print("üï∏Ô∏è Inferring topology...")
    cluster_engine = ClusteringEngine(CORRELATION_THRESHOLD)
    link_map = cluster_engine.cluster(corr_df)

    # -------------------------------
    # Confidence scoring
    # -------------------------------
    print("üìê Computing confidence scores...")
    confidences = compute_confidence(link_map, corr_df)

    # -------------------------------
    # Capacity estimation (DU/RU MODEL)
    # -------------------------------
    print("üì° Estimating Ethernet link capacity (dual capture model)...")
    capacity_engine = DualCaptureCapacityEstimator()
    capacity_map = capacity_engine.estimate(link_map, handler)

    # -------------------------------
    # Visualization
    # -------------------------------
    viz = Visualizer()

    print("üé® Generating heatmap...")
    viz.save_heatmap(
        corr_df,
        os.path.join(OUTPUT_DIR, "heatmap.png")
    )

    print("üï∏Ô∏è Generating topology graph...")
    viz.save_topology_graph(
        link_map,
        confidences,
        os.path.join(OUTPUT_DIR, "topology_graph.png")
    )

    # -------------------------------
    # Export for frontend / ML
    # -------------------------------
    print("üíæ Exporting topology JSON...")
    export_topology(
        os.path.join(OUTPUT_DIR, "topology.json"),
        link_map,
        confidences,
        CORRELATION_THRESHOLD,
        dataset_label,
        len(cells),
        capacity_map
    )

    # -------------------------------
    # Console summary
    # -------------------------------
    print("\nüèÅ DONE ‚Äî Network Topology Discovered\n")

    for link, group in link_map.items():
        conf = confidences.get(link, 0.0)
        cap = capacity_map.get(link, {})

        print(
            f"{link}: {group} | "
            f"confidence={conf:.2f} | "
            f"peak={cap.get('peak_demand_gbps', 0)} Gbps | "
            f"safe_capacity={cap.get('safe_capacity_gbps', 0)} Gbps | "
            f"congestion={cap.get('congestion_score', 0)}"
        )

    print(f"\nüßæ Frontend JSON saved to: {OUTPUT_DIR}/topology.json")
    print(f"üìä Heatmap saved to: {OUTPUT_DIR}/heatmap.png")
    print(f"üï∏Ô∏è Topology graph saved to: {OUTPUT_DIR}/topology_graph.png")


if __name__ == "__main__":
    main()
